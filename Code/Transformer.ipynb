{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import nltk\n",
    "import pickle\n",
    "import os.path\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        vocab_threshold,\n",
    "        vocab_file='./vocab.pkl',\n",
    "        start_word=0,\n",
    "        end_word=1,\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file='annotations/captions_train2014.json',\n",
    "        vocab_from_file=False):\n",
    "        \n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def get_loader(transform,\n",
    "               mode='train',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=0,\n",
    "               end_word=1,\n",
    "               unk_word=\"<unk>\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               cocoapi_loc=''):\n",
    "    \n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'images/train2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'annotations/captions_train2014.json')\n",
    "    \n",
    "    if mode == 'val':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'images/val2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'annotations/captions_val2014.json')\n",
    "    \n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(cocoapi_loc, 'images/test2014/')\n",
    "        annotations_file = os.path.join(cocoapi_loc, 'annotations/image_info_test2014.json')\n",
    "\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(transform=transform,\n",
    "                          mode=mode,\n",
    "                          batch_size=batch_size,\n",
    "                          vocab_threshold=vocab_threshold,\n",
    "                          vocab_file=vocab_file,\n",
    "                          start_word=start_word,\n",
    "                          end_word=end_word,\n",
    "                          unk_word=unk_word,\n",
    "                          annotations_file=annotations_file,\n",
    "                          vocab_from_file=vocab_from_file,\n",
    "                          img_folder=img_folder)\n",
    "\n",
    "    if mode == 'train' or mode == 'val':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        \n",
    "        data_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        data_loader = data.DataLoader(dataset=dataset,\n",
    "                                      batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print('Obtaining caption lengths...')\n",
    "            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption)#.long() #TO DO: might need to change\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "import sys\n",
    "nltk.download('punkt')\n",
    "#from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "     transforms.Resize(256),\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                          (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_type, encoded_image_size=14, fine_tune=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "        model = getattr(models, model_type)\n",
    "        resnet = model(pretrained=True)  \n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune(fine_tune)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, encoded_image_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.resnet(images)  \n",
    "        out = self.adaptive_pool(out) \n",
    "        out = torch.flatten(out,2,3) \n",
    "        out = out.permute(2, 0, 1) \n",
    "        return out\n",
    "        \n",
    "    def fine_tune(self, fine_tune=False):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[-1]: \n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "\n",
    "class MLP_init(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super(MLP_init, self).__init__()\n",
    "        \n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        \n",
    "        self.init_MLP = nn.Sequential(\n",
    "                            nn.Linear(encoder_hidden_size, decoder_hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
    "                        )\n",
    "        \n",
    "    def forward(self, h):\n",
    "        return self.init_MLP(h)\n",
    "\n",
    "\n",
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = queries.shape[0]\n",
    "        q = self.Q(queries.view(batch_size, -1, queries.shape[-1]))\n",
    "        k = self.K(keys)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = k@q.transpose(2,1)*self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = attention_weights.transpose(2,1)@v\n",
    "        return context, attention_weights\n",
    "        \n",
    "\n",
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        batch_size = queries.shape[0]\n",
    "        q = self.Q(queries.view(batch_size, -1, queries.shape[-1]))\n",
    "        k = self.K(keys)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = k@q.transpose(2,1)*self.scaling_factor\n",
    "        mask = ~torch.triu(unnormalized_attention).bool()\n",
    "        attention_weights = self.softmax(unnormalized_attention.masked_fill(mask, self.neg_inf))\n",
    "        context = attention_weights.transpose(2,1)@v\n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_heads)]) for j in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_heads)]) for j in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        \n",
    "\n",
    "        self.linear_after_causal = nn.ModuleList([nn.Linear(self.num_heads*hidden_size, hidden_size) for j in range(self.num_layers)])\n",
    "        self.linear_after_scaled = nn.ModuleList([nn.Linear(self.num_heads*hidden_size, hidden_size) for j in range(self.num_layers)])\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.layernorms1 = nn.ModuleList([nn.LayerNorm([self.hidden_size]) for i in range(self.num_layers)])\n",
    "        self.layernorms2 = nn.ModuleList([nn.LayerNorm([self.hidden_size]) for i in range(self.num_layers)])\n",
    "        self.layernorms3 = nn.ModuleList([nn.LayerNorm([self.hidden_size]) for i in range(self.num_layers)])\n",
    "\n",
    "    def forward(self, inputs, annotations):\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        inputs = inputs.long()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "        embed = embed + self.positional_encodings[:seq_len]\n",
    "        embed = self.dropout(embed)\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            concat_causal = torch.empty((batch_size, seq_len, 0), device='cuda')\n",
    "            concat_scaled = torch.empty((batch_size, seq_len, 0), device='cuda')\n",
    "            for j in range(self.num_heads):\n",
    "                new_contexts, self_attention_weights = self.self_attentions[i][j](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
    "                concat_causal = torch.cat((concat_causal, new_contexts), axis=2)\n",
    "                \n",
    "            new_contexts = self.linear_after_causal[i](concat_causal) #batch_size x seq_len x hidden_size*num_heads -----> batch_size x seq_len x hidden_size\n",
    "            new_contexts = self.dropout(new_contexts) #dropout\n",
    "            residual_contexts = self.layernorms1[i](contexts + new_contexts) #add and norm\n",
    "\n",
    "            for j in range(self.num_heads):\n",
    "                new_contexts, encoder_attention_weights = self.encoder_attentions[i][j](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
    "                concat_scaled = torch.cat((concat_scaled, new_contexts), axis=2)\n",
    "            \n",
    "            new_contexts = self.linear_after_scaled[i](concat_scaled) #batch_size x seq_len x hidden_size*num_heads -----> batch_size x seq_len x hidden_size\n",
    "            new_contexts = self.dropout(new_contexts) #dropout\n",
    "            residual_contexts = self.layernorms2[i](residual_contexts + new_contexts) #add and norm\n",
    "\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "            new_contexts = self.dropout(new_contexts) #dropout\n",
    "            contexts = self.layernorms3[i](residual_contexts + new_contexts) #add and norm\n",
    "\n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)\n",
    "          \n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, (encoder_attention_weights, self_attention_weights)\n",
    "\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "        Arguments:\n",
    "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "        Returns:\n",
    "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
    "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        #print(pos_encodings.shape)\n",
    "        pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "#Main class for Encoder and Decoder\n",
    "\n",
    "    def __init__(\n",
    "            self, encoder_class, decoder_class,\n",
    "            target_vocab_size, target_sos=-2, target_eos=-1, encoder_type='resnet18', fine_tune=False, encoder_hidden_size=512,\n",
    "            decoder_hidden_size=1024, word_embedding_size=1024, attention_dim=512, cell_type='lstm', decoder_type='rnn', beam_width=4, dropout=0.0,\n",
    "            transformer_layers=3, num_heads=1):\n",
    "        # Init Encoder and Decoder\n",
    "        super().__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.target_sos = target_sos\n",
    "        self.target_eos = target_eos\n",
    "        self.encoder_type = encoder_type\n",
    "        self.fine_tune = fine_tune\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.cell_type = cell_type\n",
    "        self.decoder_type = decoder_type\n",
    "        self.beam_width = beam_width\n",
    "        self.dropout = dropout\n",
    "        self.transformer_layers = transformer_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder = self.decoder = None\n",
    "        self.init_submodules(encoder_class, decoder_class)\n",
    "        \n",
    "    def init_submodules(self, encoder_class, decoder_class):\n",
    "        self.encoder = encoder_class(self.encoder_type, fine_tune=self.fine_tune)\n",
    "\n",
    "        self.decoder = decoder_class(self.target_vocab_size, \n",
    "                                self.encoder_hidden_size,\n",
    "                                self.transformer_layers,\n",
    "                                self.num_heads,\n",
    "                                self.dropout)\n",
    "\n",
    "    def get_target_padding_mask(self, E):\n",
    "        '''Determine what parts of a target sequence batch are padding\n",
    "        `E` is right-padded with end-of-sequence symbols. This method\n",
    "        creates a mask of those symbols, excluding the first in every sequence\n",
    "        (the first eos symbol should not be excluded in the loss).\n",
    "        Parameters\n",
    "        ----------\n",
    "        E : torch.LongTensor\n",
    "            A float tensor of shape ``(T - 1, N)``, where ``E[t', n]`` is\n",
    "            the ``t'``-th token id of a gold-standard transcription for the\n",
    "            ``n``-th source sequence. *Should* exclude the initial\n",
    "            start-of-sequence token.\n",
    "        Returns\n",
    "        -------\n",
    "        pad_mask : torch.BoolTensor\n",
    "            A boolean tensor of shape ``(T - 1, N)``, where ``pad_mask[t, n]``\n",
    "            is :obj:`True` when ``E[t, n]`` is considered padding.\n",
    "        '''\n",
    "        pad_mask = E == self.target_eos  # (T - 1, N)\n",
    "        pad_mask = pad_mask & torch.cat([pad_mask[:1], pad_mask[:-1]], 0)\n",
    "        return pad_mask\n",
    "\n",
    "    def forward(self, images, captions=None, max_T=100, on_max='raise'):\n",
    "        h = self.encoder(images)  # (L, N, H)\n",
    "        if self.training:\n",
    "            return self.get_logits_for_teacher_forcing(h, captions)\n",
    "        else:\n",
    "            return self.beam_search(h, max_T, on_max)\n",
    "\n",
    "    def get_logits_for_teacher_forcing(self, h, captions):\n",
    "        # name is not relevant\n",
    "        op = []\n",
    "        op, _ = self.decoder(captions[:-1,:].T, h.permute(1,0,2))\n",
    "        return op\n",
    "        \n",
    "    def beam_search(self, h, max_T, on_max):\n",
    "\n",
    "        # Inputs: h: encoder hidden states. #(H*W, batch_size, L) default is (196, batch_size, 2048)\n",
    "        assert not self.training\n",
    "        random_placeholder = torch.randn(h.shape[1], self.decoder_hidden_size, device=h.device)\n",
    "        logpb_tm1 = torch.where(\n",
    "            torch.arange(self.beam_width, device=h.device) > 0,  # K\n",
    "            torch.full_like(\n",
    "                random_placeholder[..., 0].unsqueeze(1), -float('inf')),  # k > 0\n",
    "            torch.zeros_like(\n",
    "                random_placeholder[..., 0].unsqueeze(1)),  # k == 0\n",
    "        )  # (N, K)\n",
    "        \n",
    "        assert torch.all(logpb_tm1[:, 0] == 0.)\n",
    "        assert torch.all(logpb_tm1[:, 1:] == -float('inf'))\n",
    "        b_tm1_1 = torch.full_like(  # (t, N, K)\n",
    "            logpb_tm1, self.target_sos, dtype=torch.float).unsqueeze(0) #Changed long to float\n",
    "        # We treat each beam within the batch as just another batch when\n",
    "        # computing logits, then recover the original batch dimension by\n",
    "        # reshaping\n",
    "        \n",
    "        h = h.unsqueeze(2).repeat(1, 1, self.beam_width, 1)\n",
    "        h = h.flatten(1, 2)  # (S, N * K, L)\n",
    "        v_is_eos = torch.arange(self.target_vocab_size, device=h.device)\n",
    "        v_is_eos = v_is_eos == self.target_eos  # (V,)\n",
    "        t = 0\n",
    "        logits_tm1 = None\n",
    "        cur_transformer_ip = None\n",
    "        while torch.any(b_tm1_1[-1, :, 0] != self.target_eos):\n",
    "            if t == max_T:\n",
    "                if on_max == 'raise':\n",
    "                    raise RuntimeError(\n",
    "                        f'Beam search has not finished by t={t}. Increase the '\n",
    "                        f'number of parameters and train longer')\n",
    "                elif on_max == 'halt':\n",
    "                    print(f'Beam search not finished by t={t}. Halted')\n",
    "                    break\n",
    "            finished = (b_tm1_1[-1] == self.target_eos)\n",
    "           \n",
    "            E_tm1 = b_tm1_1[-1].flatten().unsqueeze(1)  # (N * K, 1)\n",
    "\n",
    "            if cur_transformer_ip == None:\n",
    "                cur_transformer_ip = E_tm1\n",
    "#             except:\n",
    "            else:\n",
    "                cur_transformer_ip = torch.cat([cur_transformer_ip, E_tm1], axis=1)\n",
    "            op, _ = self.decoder(cur_transformer_ip, h.permute(1,0,2))\n",
    "            logits_t = op[:, -1, :]\n",
    "            logits_tm1 = logits_t\n",
    "            logits_t = logits_t.view(\n",
    "                -1, self.beam_width, self.target_vocab_size)  # (N, K, V)\n",
    "            logpy_t = nn.functional.log_softmax(logits_t, -1)\n",
    "            # We length-normalize the extensions of the unfinished paths\n",
    "            if t:\n",
    "                logpb_tm1 = torch.where(\n",
    "                    finished, logpb_tm1, logpb_tm1 * (t / (t + 1)))\n",
    "                logpy_t = logpy_t / (t + 1)\n",
    "            logpy_t = logpy_t.masked_fill(\n",
    "                finished.unsqueeze(-1) & v_is_eos, 0.)\n",
    "            logpy_t = logpy_t.masked_fill(\n",
    "                finished.unsqueeze(-1) & (~v_is_eos), -float('inf'))\n",
    "            if self.decoder_type == 'rnn':\n",
    "                if self.cell_type == 'lstm':\n",
    "                    htilde_t = (\n",
    "                        htilde_t[0].view(\n",
    "                            -1, self.beam_width, self.decoder_hidden_size),\n",
    "                        htilde_t[1].view(\n",
    "                            -1, self.beam_width, self.decoder_hidden_size),\n",
    "                    )\n",
    "                else:\n",
    "                    htilde_t = htilde_t.view(\n",
    "                        -1, self.beam_width, self.decoder_hidden_size)\n",
    "                b_t_0, b_t_1, logpb_t = self.update_beam(\n",
    "                    htilde_t, b_tm1_1, logpb_tm1, logpy_t)\n",
    "                del logits_t, logpy_t, finished, htilde_t\n",
    "                if self.cell_type == 'lstm':\n",
    "                    htilde_tm1 = (\n",
    "                        b_t_0[0].flatten(end_dim=1),\n",
    "                        b_t_0[1].flatten(end_dim=1)\n",
    "                    )\n",
    "                else:\n",
    "                    htilde_tm1 = b_t_0.flatten(end_dim=1)  # (N * K, 2 * H)\n",
    "            else:\n",
    "                b_t_1, logpb_t = self.update_beam(None, b_tm1_1, logpb_tm1, logpy_t)\n",
    "                del logits_t, logpy_t, finished\n",
    "            logpb_tm1, b_tm1_1 = logpb_t, b_t_1\n",
    "            t += 1\n",
    "        return b_tm1_1\n",
    "\n",
    "    def update_beam(self, htilde_t, b_tm1_1, logpb_tm1, logpy_t):\n",
    "        V = logpy_t.shape[2] #Vocab size\n",
    "        K = logpy_t.shape[1] #Beam width\n",
    "\n",
    "        s = logpb_tm1.unsqueeze(-1).expand_as(logpy_t) + logpy_t\n",
    "        logy_flat = torch.flatten(s, 1, 2)\n",
    "        top_k_val, top_k_ind = torch.topk(logy_flat, K, dim = 1)\n",
    "        temp = top_k_ind // V #This tells us which beam that top value  is from\n",
    "        logpb_t = top_k_val\n",
    "\n",
    "        temp_ = temp.expand_as(b_tm1_1)\n",
    "        b_t_1 = torch.cat((torch.gather(b_tm1_1, 2, temp_), (top_k_ind % V).unsqueeze(0)))\n",
    "\n",
    "        if htilde_t != None:\n",
    "            if(self.cell_type == 'lstm'):\n",
    "                temp_ = temp.unsqueeze(-1).expand_as(htilde_t[0])\n",
    "                b_t_0 = (torch.gather(htilde_t[0], 1, temp_), torch.gather(htilde_t[1], 1, temp_))\n",
    "            else:\n",
    "                temp_ = temp.unsqueeze(-1).expand_as(htilde_t)\n",
    "                b_t_0 = torch.gather(htilde_t, 1, temp_)\n",
    "\n",
    "            return b_t_0, b_t_1, logpb_t\n",
    "        else:\n",
    "            return b_t_1, logpb_t\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_for_epoch(model, dataloader, optimizer, device, n_iter, epoch, losses):\n",
    "    # trains for one epoch\n",
    "    \n",
    "    criterion1 = nn.CrossEntropyLoss(ignore_index=-1, reduction='sum')\n",
    "    total_loss = 0 \n",
    "    total_num = 0\n",
    "    count = 0\n",
    "    target_eos = 1\n",
    "    \n",
    "    for data in tqdm(dataloader):\n",
    "        images, captions, cap_lens = data\n",
    "        captions = pad_sequence(captions, padding_value=model.module.target_eos) #(seq_len, batch_size)\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, captions).permute(1, 0, 2)\n",
    "\n",
    "        captions = captions[1:]\n",
    "        mask = model.module.get_target_padding_mask(captions)\n",
    "        captions = captions.masked_fill(mask,-1)\n",
    "        loss = criterion1(torch.flatten(logits, 0, 1), torch.flatten(captions))\n",
    "        total_loss += float(loss.item())\n",
    "        total_num += len(cap_lens)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        optimizer.step()\n",
    "        n_iter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        if n_iter % 10 == 0:\n",
    "            # Get training statistics.\n",
    "            stats = 'Epoch %d, Step %d, Loss: %.4f, Perplexity: %5.4f' % (epoch, n_iter, total_loss/total_num, torch.exp(loss))\n",
    "            print(stats, \"\\n\")\n",
    "        \n",
    "        if n_iter % 10 == 0:\n",
    "            losses.append(total_loss/total_num)\n",
    "        \n",
    "        if n_iter % 2000 == 0:\n",
    "            torch.save(model.module.decoder.state_dict(), os.path.join('./checkpoints1', 'transformerdecoder-epoch-%d-step-%d.pkl'%(epoch,n_iter)))\n",
    "        del images\n",
    "        del captions\n",
    "    return total_loss/total_num, n_iter, losses\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "\n",
    "                \n",
    "def collate_fn(data):\n",
    "    # Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]  \n",
    "    lengths = torch.tensor(lengths)\n",
    "    \n",
    "    return images, targets, lengths\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_class = Encoder\n",
    "decoder_class = TransformerDecoder\n",
    "encoder_type = 'resnet50'\n",
    "decoder_type = 'transformer' #transformer, rnn\n",
    "warmup_steps = 4000\n",
    "n_iter = 1\n",
    "CNN_channels = 2048\n",
    "\n",
    "epoch = 0\n",
    "max_epochs = 4\n",
    "beam_width = 4\n",
    "\n",
    "print(\"Epochs are read correctly: \", max_epochs)\n",
    "print(\"Encoder type is read correctly: \", encoder_type)\n",
    "print(\"Number of CNN channels being used: \", CNN_channels)\n",
    "print(\"Fine tune setting is set to: \", bool(0))\n",
    "\n",
    "\n",
    "word_embedding_size = 512\n",
    "attention_dim = 512\n",
    "model_save_path = './model_saves/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lamda = 1.\n",
    "\n",
    "\n",
    "# print(\"Label smoothing set to: \", bool(1))   \n",
    "learning_rate = 0.00004\n",
    "decoder_hidden_size = CNN_channels\n",
    "dropout = 0.1\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_val = 64\n",
    "grad_clip = 5.\n",
    "transformer_layers = 3\n",
    "heads = 2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "mode = 'train'\n",
    "losses = []\n",
    "vocab_threshold = 5\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=64,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False,\n",
    "                         num_workers=4)\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "\n",
    "\n",
    "model = EncoderDecoder(encoder_class, decoder_class, vocab_size, target_sos=0, \n",
    "                      target_eos=1, fine_tune=bool(0), encoder_type=encoder_type, encoder_hidden_size=CNN_channels, \n",
    "                       decoder_hidden_size=decoder_hidden_size, \n",
    "                       word_embedding_size=word_embedding_size, attention_dim=attention_dim, decoder_type=decoder_type, cell_type='lstm', beam_width=beam_width, dropout=dropout,\n",
    "                       transformer_layers=transformer_layers, num_heads=heads)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0]).to(device)\n",
    "\n",
    "\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "while epoch <= max_epochs:\n",
    "        model.train()\n",
    "        loss, n_iter, losses = train_for_epoch(model, data_loader, optimizer, device, n_iter, epoch, losses)\n",
    "        print(f'Epoch {epoch}: loss={loss}')\n",
    "        epoch += 1\n",
    "        with open(\"losses_after_%d_epochs.txt\"%epoch, \"wb\") as buttz1:   #Pickling\n",
    "            pickle.dump(losses, buttz1)\n",
    "        buttz1.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_class = Encoder\n",
    "decoder_class = TransformerDecoder\n",
    "encoder_type = 'resnet50'\n",
    "decoder_type = 'transformer' #transformer, rnn\n",
    "warmup_steps = 4000\n",
    "n_iter = 1\n",
    "CNN_channels = 2048\n",
    "\n",
    "epoch = 0\n",
    "max_epochs = 4\n",
    "beam_width = 4\n",
    "\n",
    "print(\"Epochs are read correctly: \", max_epochs)\n",
    "print(\"Encoder type is read correctly: \", encoder_type)\n",
    "print(\"Number of CNN channels being used: \", CNN_channels)\n",
    "print(\"Fine tune setting is set to: \", bool(0))\n",
    "\n",
    "\n",
    "word_embedding_size = 512\n",
    "attention_dim = 512\n",
    "model_save_path = './model_saves/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lamda = 1.\n",
    "\n",
    "\n",
    "print(\"Label smoothing set to: \", bool(1))   \n",
    "learning_rate = 0.00004\n",
    "decoder_hidden_size = CNN_channels\n",
    "dropout = 0.1\n",
    "\n",
    "batch_size = 32 #CHANGE THIS WHEN YOU NEED TO\n",
    "# batch_size_val = 64\n",
    "grad_clip = 5.\n",
    "transformer_layers = 3\n",
    "heads = 4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "mode = 'test'\n",
    "losses = []\n",
    "directory = './qcheckpoints2'\n",
    "decoder_file = 'transformerdecoder-epoch-4-step-64000.pkl'\n",
    "\n",
    "transform_val = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                          (0.229, 0.224, 0.225))\n",
    "                                    ])\n",
    "\n",
    "val_loader = get_loader(transform=transform_val,\n",
    "                         mode='val',\n",
    "                         batch_size=32,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False,\n",
    "                         num_workers=4)\n",
    "\n",
    "with open('data_loader_parallel.txt', 'rb') as dl:\n",
    "    data_loader = pickle.load(dl)\n",
    "\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "model = EncoderDecoder(encoder_class, decoder_class, vocab_size, target_sos=0, \n",
    "                      target_eos=1, fine_tune=bool(0), encoder_type=encoder_type, encoder_hidden_size=CNN_channels, \n",
    "                       decoder_hidden_size=decoder_hidden_size, \n",
    "                       word_embedding_size=word_embedding_size, attention_dim=attention_dim, decoder_type=decoder_type, cell_type='lstm', beam_width=beam_width, dropout=dropout,\n",
    "                       transformer_layers=transformer_layers, num_heads=heads)\n",
    "\n",
    "\n",
    "\n",
    "model.decoder.load_state_dict(torch.load(os.path.join(directory, decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_output_sentence(model, device, images, vocab):\n",
    "    # hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        images = images.to(device)\n",
    "        target_eos = len(vocab) + 1\n",
    "        target_sos = 0\n",
    "\n",
    "        b_1 = model(images, on_max='halt')\n",
    "        captions_cand = b_1[..., 0]\n",
    "\n",
    "        cands = captions_cand.T\n",
    "        cands_list = cands.tolist()\n",
    "        for i in range(len(cands_list)): #Removes sos tags\n",
    "            cands_list[i] = list(filter((target_sos).__ne__, cands_list[i]))\n",
    "            cands_list[i] = list(filter((target_eos).__ne__, cands_list[i]))\n",
    "\n",
    "    #     hypotheses += cands_list\n",
    "    \n",
    "    return cands_list\n",
    "\n",
    "def clean_sentence(output):\n",
    "    \n",
    "    words_sequence = []\n",
    "    \n",
    "    for i in output:\n",
    "        if (i == 1):\n",
    "            continue\n",
    "        words_sequence.append(data_loader.dataset.vocab.idx2word[i])\n",
    "    \n",
    "    words_sequence = words_sequence[1:-1] \n",
    "    return words_sequence\n",
    "\n",
    "\n",
    "def get_prediction(model):\n",
    "    orig_image, images = next(iter(data_loader_test))\n",
    "    vocab = data_loader.dataset.vocab\n",
    "    plt.imshow(np.squeeze(orig_image,0))\n",
    "    plt.title('Sample Image')\n",
    "    plt.show()\n",
    "    images = images.to(device)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    print(\"images: \", images.shape)\n",
    "    sentence = get_output_sentence(model, device, images, vocab)[0]\n",
    "    sentence = clean_sentence(sentence)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open losses file and plot. You will need to replace the filename with whatever you saved your loss as\n",
    "\n",
    "with open('q2_losses_after_5_epochs.txt', 'rb') as five:\n",
    "    losses5 = pickle.load(five)\n",
    "ploss=[i.item() for i in losses5]\n",
    "plt.plot(ploss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions for getting Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_bleu(output, data_loader):\n",
    "    words_sequence = []\n",
    "    for i in output:\n",
    "        i = int(i)\n",
    "        if (i == 1):\n",
    "            continue\n",
    "        words_sequence.append(data_loader.dataset.vocab.idx2word[i])\n",
    "    words_sequence = words_sequence[1:-1] \n",
    "    return words_sequence\n",
    "\n",
    "def get_avg_bleu(model, dataloader, device):\n",
    "    '''Determine the average BLEU score across the entire dataset\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        total_score1 = 0\n",
    "        total_score2 = 0\n",
    "        total_score3 = 0\n",
    "        total_score4 = 0\n",
    "        total_num = 0\n",
    "        for data in tqdm(dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            #load images and reference captions from the dataset\n",
    "            images, captions_ref, cap_lens = data\n",
    "            # Get caption predictions for the current batch of validation images\n",
    "            captions_cand = caption_list\n",
    "            \n",
    "            #calculate the bleu score for the entire batch\n",
    "            print(captions_ref.shape)\n",
    "            refs = []\n",
    "            cands = []\n",
    "            for captions in captions_ref:\n",
    "                refs.append(clean_sentence_bleu(captions, dataloader))\n",
    "                \n",
    "            for captions_2 in captions_cand:\n",
    "                cands.append(clean_sentence_bleu(captions_2, dataloader))\n",
    "            batch_score1, batch_score2, batch_score3, batch_score4 = get_batch_bleu(refs, cands)\n",
    "            \n",
    "            #increment total_score and total_num\n",
    "            total_score1 = total_score1 + batch_score1\n",
    "            total_score2 = total_score2 + batch_score2\n",
    "            total_score3 = total_score3 + batch_score3\n",
    "            total_score4 = total_score4 + batch_score4\n",
    "            \n",
    "            total_num = total_num + dataloader.dataset.batch_size\n",
    "        \n",
    "            \n",
    "            #print('Total Num: ',total_num)\n",
    "            \n",
    "        avg_score1 = total_score1/total_num\n",
    "        avg_score2 = total_score2/total_num\n",
    "        avg_score3 = total_score3/total_num\n",
    "        avg_score4 = total_score4/total_num\n",
    "        \n",
    "        return avg_score1, avg_score2, avg_score3, avg_score4\n",
    "\n",
    "def get_batch_candidates(model, images):\n",
    "    '''Gets predictions for the current batch of images\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        captions_cand = []\n",
    "        \n",
    "        print(\"images: \", images.shape)\n",
    "        captions_cand = model(images, on_max = 'halt')\n",
    "        print(\"captions cand: \", captions_cand)\n",
    "\n",
    "        return torch.Tensor(captions_cand)\n",
    "\n",
    "def get_batch_bleu(captions_ref, captions_cand):\n",
    "    '''Compute the total BLEU score over elements in a batch\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        scores1 = 0\n",
    "        scores2 = 0\n",
    "        scores3 = 0\n",
    "        scores4 = 0\n",
    "        for i in range(len(captions_ref)):\n",
    "            #print(len(ref))\n",
    "            captions_cand[i] = [x for x in captions_cand[i] if x != '.']\n",
    "            captions_ref[i] = [x for x in captions_ref[i] if x != '.']\n",
    "            captions_cand[i] = [x for x in captions_cand[i] if x != 0]\n",
    "            captions_ref[i] = [x for x in captions_ref[i] if x != 0]\n",
    "            \n",
    "            print('captions_ref= ',captions_ref[i])\n",
    "            print('captions_cand= ',captions_cand[i])\n",
    "            scores1 += nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i], weights=(1, 0, 0, 0))\n",
    "            scores2 += nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i], weights=(0.5, 0.5, 0, 0))\n",
    "            scores3 += nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i], weights=(0.33, 0.33, 0.33, 0.33))\n",
    "            scores4 += nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i], weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            print('Score : ',nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i],weights=(1, 0, 0, 0)))\n",
    "            print('Score : ',nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i],weights=(0.5, 0.5, 0, 0)))\n",
    "            print('Score : ',nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i],weights=(0.33, 0.33, 0.33, 0)))\n",
    "            print('Score : ',nltk.translate.bleu_score.sentence_bleu([captions_ref[i]], captions_cand[i],weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "        return scores1/batch_size, scores2/batch_size, scores3/batch_size, scores4/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, captions_ref, cap_lens = next(iter(val_loader))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "caption_list = []\n",
    "references = []\n",
    "vocab = data_loader.dataset.vocab\n",
    "count = 0\n",
    "num_batches = 100 #Determine over how many batches you would like to calculate BLEU score\n",
    "\n",
    "for data in tqdm(val_loader):\n",
    "    images, captions_ref, cap_lens = data\n",
    "    sublist = []\n",
    "    for image in images:\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        sentence = get_output_sentence(model, device, image, vocab)[0]\n",
    "        try:\n",
    "            sentence = clean_sentence(sentence)\n",
    "            sublist.append(sentence)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    caption_list.append(sublist)\n",
    "    references.append(captions_ref)\n",
    "    count += 1\n",
    "    if count == num_batches:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get BLEU scores\n",
    "\n",
    "bleu_list = []\n",
    "bleus = [0,0,0,0]\n",
    "for i in range(4000): \n",
    "    refs = []\n",
    "    for caption in references[i]:\n",
    "        refs.append(clean_sentence_bleu(caption, val_loader))\n",
    "    bleu1, bleu2, bleu3, bleu4 = get_batch_bleu(refs, caption_list[i])\n",
    "    bleus[0] += bleu1\n",
    "    bleus[1] += bleu2\n",
    "    bleus[2] += bleu3\n",
    "    bleus[3] += bleu4\n",
    "    bleu_list.append([bleu1, bleu2, bleu3, bleu4])\n",
    "\n",
    "for i in range(len(bleus)):\n",
    "    bleus[i] = bleus[i]/4000\n",
    "\n",
    "print(bleus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
